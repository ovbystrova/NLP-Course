{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мастер и Маргарита. Разбиение на предложения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных и преодобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = 'master_and_margo.txt'\n",
    "sample = 'sample.txt'\n",
    "sample_init = 'sample_init.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отрывок из текста был разбит вручную. В файле sample.txt каждое новое предложение написано с новой строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text - полный текст\n",
    "#sample_text - отрывок, разбитый на предложения\n",
    "#init_text - исходный отрывок\n",
    "with open(book, encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "with open(sample, encoding = 'utf-8') as f:\n",
    "    sample_text = f.readlines()\n",
    "with open(sample_init, encoding = 'utf-8') as f:\n",
    "    init_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Каждый текст был почищен от лишних символом; если подряд шло два пробела, они были заменены на один\n",
    "for i in range(len(sample_text)):\n",
    "    sample_text[i] = re.sub(r'\\xa0', ' ', sample_text[i])\n",
    "    sample_text[i] = re.sub(r'  ', ' ', sample_text[i])\n",
    "    sample_text[i] = re.sub(r'\\n', '', sample_text[i])\n",
    "\n",
    "init_text = re.sub(r'\\n', ' ', init_text)\n",
    "text =re.sub(r'\\n',' ', text)\n",
    "text = re.sub(r'\\t', '', text)\n",
    "\n",
    "init_text = re.sub(r'\\xa0', ' ', init_text)\n",
    "text =re.sub(r'\\xa0',' ', text)\n",
    "\n",
    "text = re.sub(r'  ', ' ', text)\n",
    "init_text = re.sub(r'  ', ' ', init_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['На дверях комнаты N 2 было написано что-то не совсем понятное: «Однодневная творческая путевка.', 'Обращаться к М. В. Подложной».', 'Следующая дверь несла на себе краткую, но уже вовсе непонятную надпись: «Перелыгино».', 'Потом у случайного посетителя Грибоедова начинали разбегаться глаза от надписей, пестревших на ореховых теткиных дверях: «Запись в очередь на бумагу у Поклевкиной», «Касса», «Личные расчеты скетчистов»...', 'Прорезав длиннейшую очередь, начинавшуюся уже внизу в швейцарской, можно было видеть надпись на двери, в которую ежесекундно ломился народ: «Квартирный вопрос».']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sample_text[:5])\n",
    "len(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'На дверях комнаты N 2 было написано что-то не совсем понятное: «Однодневная творческая путевка. Обращаться к М. В. Подложной». Следующая дверь несла на себе краткую, но уже вовсе непонятную надпись: «Перелыгино». Потом у случайного посетителя Грибоедова'"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_text[:253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ЧАСТЬ ПЕРВАЯ ...Так кто ж ты, наконец? – Я – часть той силы, что вечно хочет зла и вечно совершает благо.   Гете. «Фауст»  Глава 1 Никогда не разговаривайте с неизвестными Однажды весною, в час небывало жаркого заката, в Москве, на Патриарших прудах, появились два гражданина. Первый из них, одетый в летнюю серенькую пару, был маленького роста, упитан, лыс, свою приличную шляпу пирожком нес в руке, а на хорошо выбритом лице его помещались сверхъестественных размеров очки в черной роговой оправе. '"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длина открывка, с которым работаем - 39 предложений. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задаем метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За основу были взяты метрики из ноутбука с семинара(https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/Sentence_tokenizer.ipynb).\n",
    "Далее идет сравнение по количеству предложений в исходном тексте и в итоговом. В функцию передается параметр funct='regexp' если анализируем работу регулярных выражений или funct='pkt' если анализируем PunktSentenceTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(gold, funct, regexp=None):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    \n",
    "    if funct=='regexp':\n",
    "    \n",
    "        for sent in gold:\n",
    "            if len(re.split(regex, sent)) == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        for i in range(len(gold)-1):\n",
    "            sent1, sent2 = gold[i], gold[i+1]\n",
    "            sent = ' '.join([sent1, sent2])\n",
    "            if len(re.split(regex, sent)) == 2:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    \n",
    "    if funct =='pkt':\n",
    "        for sent in gold:\n",
    "            if len(tokenizer.tokenize(sent)) == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        for i in range(len(gold)-1):\n",
    "            sent1, sent2 = gold[i], gold[i+1]\n",
    "            sent = ' '.join([sent1, sent2])\n",
    "            if len(tokenizer.tokenize(sent)) == 2:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    precision = (tp/(tp+fp))\n",
    "    recall = (tp/(tp+fn))\n",
    "    f1 = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "def print_metrics(p,r,f):\n",
    "    print('Precision - ', p)\n",
    "    print('Recall - ', r)\n",
    "    print('F1 - ', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Простое разбиение части текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['На дверях комнаты N 2 было написано что-то не совсем понятное: «Однодневная творческая путевка.',\n",
       " 'Обращаться к М.',\n",
       " 'В.',\n",
       " 'Подложной».',\n",
       " 'Следующая дверь несла на себе краткую, но уже вовсе непонятную надпись: «Перелыгино».',\n",
       " 'Потом у случайного посетителя Грибоедова начинали разбегаться глаза от надписей, пестревших на ореховых теткиных дверях: «Запись в очередь на бумагу у Поклевкиной», «Касса», «Личные расчеты скетчистов»...',\n",
       " 'Прорезав длиннейшую очередь, начинавшуюся уже внизу в швейцарской, можно было видеть надпись на двери, в которую ежесекундно ломился народ: «Квартирный вопрос».',\n",
       " 'За квартирным вопросом открывался роскошный плакат, на котором изображена была скала, а по гребню ее ехал всадник в бурке и с винтовкой за плечами.',\n",
       " 'Пониже – пальмы и балкон, на балконе – сидящий молодой человек с хохолком, глядящий куда-то ввысь очень-очень бойкими глазами и держащий в руке самопишущее перо.',\n",
       " 'Подпись: «Полнообъемные творческие отпуска от двух недель (рассказ-новелла) до одного года (роман, трилогия).']"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex =r'(?<=[.?!]) (?=[А-Я])'\n",
    "simple = re.split(regex, init_text)\n",
    "simple[:10]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_simple = len(simple)\n",
    "len_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.9850746268656716\n",
      "Recall -  0.868421052631579\n",
      "F1 -  0.923076923076923\n"
     ]
    }
   ],
   "source": [
    "p_simple, r_simple, f_simple = metrics(sample_text, regexp=regex, funct='regexp')\n",
    "print_metrics(p_simple, r_simple, f_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что простого разбиения не абсолютно не достаточно. Оно не справляется с инициалами ('Обращаться к М.','В.','Подложной».' - 3 предложения вместо одного). Также, такое разбиение не учитывает диалоги, прямую речь. В результате вместо 39 предложений мы получаем 33 (и это учитывая, что, как минимум, 2 лишних предложения были добавлены из-за инициалов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучаем PunktSentenceTokenizer на всем тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ус', '«м»', 'э.»', '«н', 'м'}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "print(tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применяем PunktSentenceTokenizer для части текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tokenizer.tokenize(init_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.9722222222222222\n",
      "Recall -  0.9333333333333333\n",
      "F1 -  0.9523809523809524\n"
     ]
    }
   ],
   "source": [
    "p_pst, r_pst, f_pst = metrics(sample_text, funct='pkt')\n",
    "print_metrics(p_pst, r_pst, f_pst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_pst = len(res)\n",
    "len_pst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применения PunktSentenceTokenizer дало намного лучший результат. Однако, не идеальный. Вместо 39 предложений мы получили 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Length(True=39)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simple RegExpr</th>\n",
       "      <td>0.985075</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PunktSentenceTokenizer</th>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Precision    Recall        F1  Length(True=39)\n",
       "Simple RegExpr           0.985075  0.868421  0.923077               33\n",
       "PunktSentenceTokenizer   0.972222  0.933333  0.952381               40"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Сравним полученные метрики:\n",
    "import pandas as pd\n",
    "data = [[p_simple, r_simple, f_simple, len_simple],\n",
    "        [p_pst, r_pst, f_pst, len_pst]]\n",
    "pd.DataFrame(data,  columns = ['Precision', 'Recall', 'F1', 'Length(True=39)'], index = ['Simple RegExpr', 'PunktSentenceTokenizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметно улучшение качества разбиения. Хоть точность и снизилась на 0.1, полнота и F-мера показывают значительное улучшения качества работы сегментатора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с ошибками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим подробнее ошибочные случаи (текст был вручную сравнен с идеальным разбиением):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Потом у случайного посетителя Грибоедова начинали разбегаться глаза от надписей, пестревших на ореховых теткиных дверях: «Запись в очередь на бумагу у Поклевкиной», «Касса», «Личные расчеты скетчистов»... Прорезав длиннейшую очередь, начинавшуюся уже внизу в швейцарской, можно было видеть надпись на двери, в которую ежесекундно ломился народ: «Квартирный вопрос».'"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[3]#Здесь нет разделения после многоточия, хотя оно должно быть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['– Умеешь ты жить, Амвросий!',\n",
       " '– со вздохом отвечал тощий, запущенный, с карбункулом на шее Фока румяногубому гиганту, золотистоволосому, пышнощекому Амвросию-поэту.']"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[23:25]#Здесь прямая речь отделена от слов автора, хотя так не должно быть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Оревуар, Фока!', '– и, напевая, Амвросий устремлялся к веранде под тентом.']"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[35:37]#Аналогично"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, добавив какие-либо сокращения, эти ошибки не исчезнут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#исправляем случаи с многоточием\n",
    "res2 = []\n",
    "regellipsis = r'(?<=\\.{3}) (?=[А-Я])'\n",
    "for r in res:\n",
    "    if '...' in r:\n",
    "        splited = re.split(regellipsis, r)\n",
    "        res2.extend(splited)\n",
    "    else:\n",
    "        res2.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Потом у случайного посетителя Грибоедова начинали разбегаться глаза от надписей, пестревших на ореховых теткиных дверях: «Запись в очередь на бумагу у Поклевкиной», «Касса», «Личные расчеты скетчистов»...',\n",
       " 'Прорезав длиннейшую очередь, начинавшуюся уже внизу в швейцарской, можно было видеть надпись на двери, в которую ежесекундно ломился народ: «Квартирный вопрос».']"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "#исправляем диалоги\n",
    "res3 = []\n",
    "regexdialogs='\\– [а-я]'\n",
    "for i in range(len(res2)-1):\n",
    "    sent1, sent2 = res2[i], res2[i+1]\n",
    "    if re.match(regexdialogs, sent2):\n",
    "        sent = ' '.join([sent1, sent2])\n",
    "        res3.append(sent)\n",
    "    else:\n",
    "        try:\n",
    "            if res3[-1].endswith(sent1):\n",
    "                    pass\n",
    "            else:\n",
    "                res3.append(sent1)\n",
    "        except:\n",
    "            res3.append(sent1)\n",
    "if res2[-1].startswith('- '):\n",
    "    pass\n",
    "else:\n",
    "    res3.append(res2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Посмотрим на пересечение множеств идеальных предложений и полученных итоговых res3\n",
    "len(set(sample_text) & set(res3)) / len(set(sample_text) | set(res3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого, получили идеальное разбиение путем изначальной сегментации с помощью PunktSentenceTokenizer и дальнейшими корректировками с помощью регулярных выражений."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
